{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "#pip install -U sentence-transformers\n",
    "from termcolor import colored\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import regex as re\n",
    "import zipfile\n",
    "import gc\n",
    "from scipy.stats import boxcox\n",
    "import sys \n",
    "from collections import Counter \n",
    "from tqdm import tqdm \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings, math\n",
    "from termcolor import colored\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# for eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "# for EMB\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import scipy\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PATH = \"D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Oct_Forth_projectType.csv\"\n",
    "df = pd.read_csv(DF_PATH)\n",
    "\n",
    "PROJECTS_DF_PATH = \"D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\projects.csv\"\n",
    "projects_df = pd.read_csv(PROJECTS_DF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "print('# donations in Train set: ', f\"{len(df_train):,}\")\n",
    "print('# donations in Test set: ',  f\"{len(df_test):,}\")\n",
    "\n",
    "print('# Donors in Train set: ', f\"{len(df_train['Donor ID'].unique()):,}\")\n",
    "print('# Donors in Test set: ',  f\"{len(df_test['Donor ID'].unique()):,}\")\n",
    "\n",
    "\n",
    "print('# Donors in both Train and Test sets - the ones we choose for evaluation: ',\n",
    "      colored(f\"{len(df_train[df_train['Donor ID'].isin(df_test['Donor ID'].values.tolist())]['Donor ID'].unique()):,}\", 'blue'))\n",
    "\n",
    "\n",
    "df_train = df_train[df_train['Donor ID'].isin(df_test['Donor ID'].values.tolist())].reset_index(drop=True)\n",
    "df_test = df_test[df_test['Donor ID'].isin(df_train['Donor ID'].values.tolist())].reset_index(drop=True)\n",
    "\n",
    "# sum of donation in a grouped by donor id dataset\n",
    "df_main_donor_index = df[df['Donor ID'].isin(df_test['Donor ID'])].groupby(by = ['Donor ID', 'Project ID']).sum()[['Donation Amount']].reset_index().set_index('Donor ID')\n",
    "df_train_donor_index = df_train.groupby(by = ['Donor ID', 'Project ID']).sum()[['Donation Amount']].reset_index().set_index('Donor ID')\n",
    "df_test_donor_index = df_test.groupby(by = ['Donor ID', 'Project ID']).sum()[['Donation Amount']].reset_index().set_index('Donor ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Embeddings ==================== #\n",
    "\n",
    "# ----------------------- projects profiles------------------- #\n",
    "def get_project_profile_emb(project_id: str, embeddings):\n",
    "\n",
    "    # get the ids\n",
    "    idx = projects_id.index(project_id)\n",
    "    project_profile = embeddings[idx:idx+1]\n",
    "    \n",
    "    return project_profile\n",
    "\n",
    "\n",
    "\n",
    "def get_projects_profiles_emb(ids: pd.Series, embeddings):\n",
    "\n",
    "\n",
    "    profiles_list = [get_project_profile_emb(project_id, embeddings)[0] for project_id in np.ravel([ids])]\n",
    "    project_profiles = np.vstack(profiles_list)\n",
    "\n",
    "    return project_profiles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------- Donors profiles------------------- #\n",
    "\n",
    "def build_donors_profile_emb(donor_id: str, df_train_donor_index: pd.DataFrame):\n",
    "\n",
    "    # get the id of each person and the projects they\n",
    "    # donated to\n",
    "    donations_donor_df = df_train_donor_index.loc[donor_id]\n",
    "\n",
    "\n",
    "    # get the vectors of projects this person has donated to\n",
    "    donor_donated_project_intrain_profiles = get_projects_profiles_emb(donations_donor_df['Project ID'], embeddings)\n",
    "\n",
    "\n",
    "    # get the smoothed donated amount as the weight of each project\n",
    "    donor_project_strengths = np.array(donations_donor_df['Donation Amount']).reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # multiply the weights and tfidf vectors\n",
    "    multiplication = np.multiply(donor_donated_project_intrain_profiles, donor_project_strengths)\n",
    "\n",
    "\n",
    "    # now we normalize the whole vector \n",
    "    normalized_donor_preference = preprocessing.normalize(np.sum(multiplication, axis=0).reshape(1, -1))\n",
    "    \n",
    "\n",
    "\n",
    "    return normalized_donor_preference\n",
    "\n",
    "\n",
    "def build_donors_profiles_emb(df_test_donor_index: pd.DataFrame, df_train_donor_index: pd.DataFrame, sample_size: int = None):\n",
    "    \n",
    "    # now for all donors we build a profile in a dictionary\n",
    "    donor_profiles = {}\n",
    "    donors_in_test_set = df_test_donor_index.index.unique().tolist()[:sample_size]\n",
    "\n",
    "    for donor_id in tqdm(donors_in_test_set[:sample_size], position=0, leave=True):\n",
    "        donor_profiles[donor_id] = build_donors_profile_emb(donor_id, df_train_donor_index)\n",
    "\n",
    "    return donor_profiles\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = projects_df[projects_df['Project ID'].isin(df['Project ID'])].reset_index(drop=True)\n",
    "\n",
    "project_txt = projects_df.loc[:, 'project_txt']\n",
    "projects_id = projects_df['Project ID'].tolist()\n",
    "\n",
    "\n",
    "# ----------------- embeddings---------------------#\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1') # , device= 'cuda'\n",
    "embeddings = model.encode(project_txt, batch_size=256, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_profiles_emb = build_donors_profiles_emb(df_test_donor_index, df_train_donor_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- Embeddings----------------- #\n",
    "\n",
    "class ContentBasedRecommenderWithEmbeddings:\n",
    "    MODEL_NAME = 'Content-Based-Embeddings'\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Recommending projects based on the embedding of the project description\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, donor_profiles: dict, embeddings: np.ndarray):\n",
    "        \n",
    "        self.donor_profiles = donor_profiles\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.MODEL_NAME\n",
    "\n",
    "    def _get_similar_projects_to_donor_profile(self, donor_id, top_n=1000):\n",
    "        \n",
    "\n",
    "        cosine_similarities = cosine_similarity(self.donor_profiles[donor_id], self.embeddings)\n",
    "\n",
    "\n",
    "        # sort them and get the indices\n",
    "        similar_indices = cosine_similarities.argsort()[::-1].flatten()[:top_n]\n",
    "        \n",
    "\n",
    "        # get the id of project and the score of it\n",
    "        similar_projects = sorted([(projects_id[i], cosine_similarities[0, i]) for i in similar_indices], key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "        \n",
    "        return similar_projects\n",
    "\n",
    "\n",
    "    def recommend_projects(self, donor_id: str, projects_to_ignore: list = [], top_n=10, df_test: pd.DataFrame = df_test):\n",
    "\n",
    "\n",
    "        donated_projects_test = df_test_donor_index.loc[donor_id, 'Project ID']\n",
    "        if type(donated_projects_test) == str:\n",
    "            donated_projects_test  = [donated_projects_test]\n",
    "        else:\n",
    "            donated_projects_test  = list(donated_projects_test .values)\n",
    "        \n",
    "\n",
    "\n",
    "        similar_projects = self._get_similar_projects_to_donor_profile(donor_id, top_n)\n",
    "\n",
    "\n",
    "\n",
    "        # remove projects from ignore list if it is also in the test set\n",
    "        wanted_in_test = []\n",
    "\n",
    "        for project_id in projects_to_ignore:\n",
    "            if project_id in donated_projects_test:\n",
    "                wanted_in_test.append(project_id)\n",
    "        \n",
    "        if wanted_in_test != []: projects_to_ignore = set(projects_to_ignore).difference(wanted_in_test)\n",
    "\n",
    "        \n",
    "        similar_projects_filtered = [x for x in similar_projects if x[0] not in projects_to_ignore]\n",
    "\n",
    "        recommendations_df = pd.DataFrame(similar_projects_filtered, columns = ['Project ID', 'recommStrength']).head(top_n)\n",
    "\n",
    "        recommendations_df = pd.merge(left=recommendations_df, right=projects_df, how = 'left')[['recommStrength',  'Project ID', 'Project Title', 'Project Need Statement']]\n",
    "\n",
    "        return recommendations_df, projects_to_ignore\n",
    "\n",
    "model = ContentBasedRecommenderWithEmbeddings(donor_profiles_emb, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projects_donated(donor_id: str, df_donor_indexed: pd.DataFrame) -> set:\n",
    "\n",
    "    \"\"\" \n",
    "    get the project one has donated to in a specific df\n",
    "    \"\"\"\n",
    "    try:\n",
    "        donated_projects = df_donor_indexed.loc[donor_id]['Project ID']\n",
    "\n",
    "        return set(donated_projects if type(donated_projects) == pd.Series else [donated_projects])\n",
    "        \n",
    "    except KeyError:\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-N accuracy metrics \n",
    "EVAL_RANDOM_SAMPLE_NON_INTERACTED_PROJECTS = 100\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "\n",
    "    def __init__(self, df_main_donor_index, projects_id):\n",
    "\n",
    "        self.df_main_donor_index = df_main_donor_index\n",
    "        self.df_test_donor_index = df_test_donor_index\n",
    "        self.df_train_donor_index = df_train_donor_index\n",
    "        self.projects_id = projects_id\n",
    "        \n",
    "\n",
    "\n",
    "    def get_not_donated_projects_sample(self, donor_id: str, sample_size: int, seed=42) -> set:\n",
    "        \n",
    "        \"\"\"\n",
    "        input: donor_id\n",
    "        output: a set of not donated projects in df\n",
    "        \"\"\"\n",
    "        \n",
    "        donated_projects = get_projects_donated(donor_id=donor_id, df_donor_indexed=self.df_main_donor_index)\n",
    "\n",
    "        not_donated_projects = [x for x in self.projects_id if x not in donated_projects]\n",
    "        not_donated_projects_sample = random.sample(not_donated_projects, sample_size)\n",
    "        \n",
    "        return set(not_donated_projects_sample)\n",
    "\n",
    "\n",
    "\n",
    "    def _verify_hit_top_n(self, project_id: str, recommended_projects: pd.Series, top_n) -> (bool, int):\n",
    "        \"\"\" \n",
    "        input: one project id (a project our donor has donated to), a set of recommended projects\n",
    "        output: the index of that project_id among all the recomms\n",
    "        \"\"\"\n",
    "        try:\n",
    "            index = next(i for i, c in enumerate(recommended_projects) if c == project_id)\n",
    "        except:\n",
    "            index = -1\n",
    "\n",
    "        hit = int(index in range(0, top_n))\n",
    "\n",
    "        return hit, index\n",
    "        \n",
    "\n",
    "    def evaluate_model_for_donor(self, model, donor_id: str):\n",
    "        \"\"\"\n",
    "        evaluates the recommendations recommended to one donor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # what donor has donated to in the test set\n",
    "        if type(self.df_test_donor_index.loc[donor_id, 'Project ID']) == pd.Series:\n",
    "            donated_projects_test = set(self.df_test_donor_index.loc[donor_id, 'Project ID'])\n",
    "        else:\n",
    "            donated_projects_test = set([self.df_test_donor_index.loc[donor_id, 'Project ID']])\n",
    "\n",
    "        donated_projects_count_test = len(donated_projects_test)\n",
    "\n",
    "        # overlap\n",
    "            \n",
    "        df_train_d = df_train_donor_index.loc[donor_id, 'Project ID']\n",
    "        df_test_d = df_test_donor_index.loc[donor_id, 'Project ID']\n",
    "\n",
    "        if type(df_test_d) == str: \n",
    "            df_test_d = [df_test_d]\n",
    "        else: df_test_d = list(df_test_d.values)\n",
    "\n",
    "\n",
    "        if type(df_train_d) == str: \n",
    "            df_train_d = [df_train_d]\n",
    "        else: df_train_d = list(df_train_d.values)\n",
    "\n",
    "        overlap = 0\n",
    "        for project_id in df_test_d:\n",
    "            if project_id in df_train_d:\n",
    "                overlap+=1\n",
    "            else: continue\n",
    "\n",
    "\n",
    "        recommendations_df, projects_to_ignore = model.recommend_projects(donor_id, projects_to_ignore= get_projects_donated(donor_id, \n",
    "        df_donor_indexed = df_train_donor_index), top_n = None, df_test=df_test)\n",
    "\n",
    "        # if correct we have ranked all the projects except the ones the donor have been only in training set of \n",
    "        # this specific donor\n",
    "        assert len(recommendations_df) == len(projects_id) - len(projects_to_ignore)\n",
    "\n",
    "\n",
    "        \n",
    "        hits_at_3_count = 0\n",
    "        hits_at_5_count = 0\n",
    "        hits_at_10_count = 0\n",
    "\n",
    "         \n",
    "          \n",
    "        for project_id in donated_projects_test:\n",
    "\n",
    "\n",
    "            # first get a sample of the ones he\\she has not donated to \n",
    "            not_donated_projects_sample =  self.get_not_donated_projects_sample(donor_id, sample_size = EVAL_RANDOM_SAMPLE_NON_INTERACTED_PROJECTS, seed = 42)\n",
    "                    \n",
    "                    \n",
    "            # add a donated project to a list of 100 projects this donor has not interacted with\n",
    "            validation_projects = not_donated_projects_sample.union(set([project_id]))\n",
    "\n",
    "\n",
    "            # if true means we have correctly identified the projects they have not interacted with\n",
    "            assert len(not_donated_projects_sample)+1 == len(validation_projects)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            recommendations_df_ =  recommendations_df[recommendations_df['Project ID'].isin(validation_projects)].reset_index(drop=True)\n",
    " \n",
    "\n",
    "            recommended_project_ids = recommendations_df_['Project ID'].values\n",
    "            assert len(recommended_project_ids) == 101 , print(len(recommended_project_ids))\n",
    "\n",
    "\n",
    "            hit_at_3, index_at_3 = self._verify_hit_top_n(project_id, recommended_project_ids, 3)\n",
    "            hits_at_3_count += hit_at_3\n",
    "\n",
    "            hit_at_5, index_at_5 = self._verify_hit_top_n(project_id, recommended_project_ids, 5)\n",
    "            hits_at_5_count += hit_at_5\n",
    "\n",
    "            hit_at_10, index_at_10 = self._verify_hit_top_n(project_id, recommended_project_ids, 10)\n",
    "            hits_at_10_count += hit_at_10\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------- Recall --------------------------#\n",
    "        recall_at_3 = hits_at_3_count/float(donated_projects_count_test)\n",
    "        recall_at_5 = hits_at_5_count / float(donated_projects_count_test)\n",
    "        recall_at_10 = hits_at_10_count / float(donated_projects_count_test)\n",
    "\n",
    "\n",
    "        donor_metrics = {'donor_id': donor_id,\n",
    "                        'hits@3_count':hits_at_3_count, \n",
    "                         'hits@5_count':hits_at_5_count, \n",
    "                          'hits@10_count':hits_at_10_count, \n",
    "                          'donated_count': donated_projects_count_test,\n",
    "                          'project_overlap_test_train_count': overlap,\n",
    "                          'recall@3': recall_at_3,\n",
    "                          'recall@5': recall_at_5,\n",
    "                          'recall@10': recall_at_10}\n",
    "        return donor_metrics\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        aggregates the results of evaluate_model_for_donor\n",
    "        \"\"\"\n",
    "        metrics = []\n",
    "\n",
    "        for idx, donor_id in enumerate(list(self.df_test_donor_index.index.unique().values)):\n",
    "            \n",
    "            donor_metrics = self.evaluate_model_for_donor(model, donor_id)\n",
    "            if idx%500 ==0: print('%d donors processed' % idx)\n",
    "\n",
    "            metrics.append(donor_metrics)\n",
    "\n",
    "            detailed_results_df = pd.DataFrame(metrics).sort_values('donated_count',  ascending=False).reset_index(drop=True)\n",
    "            \n",
    "        glob_num_donations = float(detailed_results_df['donated_count'].sum())\n",
    "\n",
    "        global_recall_at_3 = detailed_results_df['hits@3_count'].sum()/ glob_num_donations\n",
    "        global_recall_at_5 = detailed_results_df['hits@5_count'].sum()/ glob_num_donations\n",
    "        global_recall_at_10 = detailed_results_df['hits@10_count'].sum()/ glob_num_donations\n",
    "\n",
    "        global_metrics = {'modelName': model.get_model_name(),\n",
    "                          'recall@3': global_recall_at_3,\n",
    "                          'recall@5': global_recall_at_5,\n",
    "                          'recall@10': global_recall_at_10}    \n",
    "        return global_metrics, detailed_results_df\n",
    "    \n",
    "\n",
    "model_evaluator = ModelEvaluator(df_main_donor_index, projects_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored('Embedding- and content-based: ', 'green'))\n",
    "global_metrics, detailed_results_df = model_evaluator.evaluate_model(model)\n",
    "\n",
    "\n",
    "print('\\nGlobal metrics:\\n%s' % global_metrics)\n",
    "detailed_results_df = detailed_results_df[['donor_id', 'donated_count', \"hits@3_count\", 'hits@5_count', 'hits@10_count', 'recall@3','recall@5','recall@10']]\n",
    "detailed_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
