{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "#pip install -U sentence-transformers\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import regex as re\n",
    "import zipfile\n",
    "import gc\n",
    "from scipy.stats import boxcox\n",
    "import sys \n",
    "from collections import Counter \n",
    "from tqdm import tqdm \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings, math\n",
    "from termcolor import colored\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# for eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "# for SVD\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix\n",
    "pd.set_option('display.float_format','{:.5f}'.format)\n",
    "\n",
    "# for EMB\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import scipy\n",
    "from sklearn import preprocessing \n",
    "\n",
    "# for TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from sklearn import preprocessing \n",
    "\n",
    "# for creating a network\n",
    "from sklearn.cluster import KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXT_FEATURES_PATH = 'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\ext_features_agg.csv'\n",
    "NEW_DF_PATH = 'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\Oct_28_df.csv'\n",
    "\n",
    "ext_features = pd.read_csv(EXT_FEATURES_PATH)\n",
    "df = pd.read_csv(NEW_DF_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = projects_df[projects_df['Project ID'].isin(df['Project ID'])].reset_index(drop=True)\n",
    "\n",
    "project_txt = projects_df.loc[:, 'project_txt']\n",
    "projects_id = projects_df['Project ID'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- embeddings---------------------#\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1') # , device= 'cuda'\n",
    "embeddings = model.encode(project_txt, batch_size=256, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMBEDDING_PATH, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donor Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_PATH = 'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\Oct_28_project_embeddings.pickle'\n",
    "project_embeddings = pd.read_pickle(EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Embeddings ==================== #\n",
    "\n",
    "# ----------------------- projects profiles------------------- #\n",
    "def get_project_profile_emb(project_id: str, project_embeddings):\n",
    "\n",
    "    # get the ids\n",
    "    idx = projects_id.index(project_id)\n",
    "    project_profile = project_embeddings[idx:idx+1]\n",
    "    \n",
    "    return project_profile\n",
    "\n",
    "\n",
    "\n",
    "def get_projects_profiles_emb(ids: pd.Series, project_embeddings):\n",
    "\n",
    "\n",
    "    profiles_list = [get_project_profile_emb(project_id, project_embeddings)[0] for project_id in np.ravel([ids])]\n",
    "    project_profiles = np.vstack(profiles_list)\n",
    "\n",
    "    return project_profiles\n",
    "\n",
    "\n",
    "# ----------------------- Donors profiles------------------- #\n",
    "\n",
    "def build_donors_profile_emb(donor_id: str, df_train_donor_index: pd.DataFrame):\n",
    "\n",
    "    # get the id of each person and the projects they\n",
    "    # donated to\n",
    "    donations_donor_df = df_train_donor_index.loc[donor_id]\n",
    "\n",
    "\n",
    "    # get the vectors of projects this person has donated to\n",
    "    donor_donated_project_intrain_profiles = get_projects_profiles_emb(donations_donor_df['Project ID'], project_embeddings)\n",
    "\n",
    "\n",
    "    # get the smoothed donated amount as the weight of each project\n",
    "    donor_project_strengths = np.array(donations_donor_df['Donation Amount']).reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # multiply the weights and tfidf vectors\n",
    "    multiplication = np.multiply(donor_donated_project_intrain_profiles, donor_project_strengths)\n",
    "\n",
    "\n",
    "    # now we normalize the whole vector \n",
    "    normalized_donor_preference = preprocessing.normalize(np.sum(multiplication, axis=0).reshape(1, -1))\n",
    "    \n",
    "\n",
    "\n",
    "    return normalized_donor_preference\n",
    "\n",
    "\n",
    "def build_donors_profiles_emb(df_test_donor_index: pd.DataFrame, df_train_donor_index: pd.DataFrame, sample_size: int = None):\n",
    "    \n",
    "    # now for all donors we build a profile in a dictionary\n",
    "    donor_profiles = {}\n",
    "    donors_in_test_set = df_test_donor_index.index.unique().tolist()[:sample_size]\n",
    "\n",
    "    for donor_id in tqdm(donors_in_test_set[:sample_size], position=0, leave=True):\n",
    "        donor_profiles[donor_id] = build_donors_profile_emb(donor_id, df_train_donor_index)\n",
    "\n",
    "    return donor_profiles\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_profiles_emb = build_donors_profiles_emb(df_test_donor_index, df_train_donor_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
