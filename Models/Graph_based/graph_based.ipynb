{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "#pip install -U sentence-transformers\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import regex as re\n",
    "import zipfile\n",
    "import gc\n",
    "from scipy.stats import boxcox\n",
    "import sys \n",
    "from collections import Counter \n",
    "from tqdm import tqdm \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings, math\n",
    "from termcolor import colored\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# for eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "# for SVD\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix\n",
    "pd.set_option('display.float_format','{:.5f}'.format)\n",
    "\n",
    "# for EMB\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import scipy\n",
    "from sklearn import preprocessing \n",
    "\n",
    "# for TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from sklearn import preprocessing \n",
    "\n",
    "# for creating a network\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXT_FEATURES_PATH = 'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\ext_features_agg.csv'\n",
    "NEW_DF_PATH = 'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\Oct_28_df.csv'\n",
    "PROJECTS_DF_PATH = \"D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\projects.csv\"\n",
    "\n",
    "\n",
    "ext_features = pd.read_csv(EXT_FEATURES_PATH)\n",
    "df = pd.read_csv(NEW_DF_PATH)\n",
    "projects_df = pd.read_csv(PROJECTS_DF_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# donations in Train set:  43,604\n",
      "# donations in Test set:  4,845\n",
      "# Donors in Train set:  790\n",
      "# Donors in Test set:  617\n",
      "# Donors in both Train and Test sets - the ones we choose for evaluation:  \u001b[34m617\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "print('# donations in Train set: ', f\"{len(df_train):,}\")\n",
    "print('# donations in Test set: ',  f\"{len(df_test):,}\")\n",
    "\n",
    "print('# Donors in Train set: ', f\"{len(df_train['Donor ID'].unique()):,}\")\n",
    "print('# Donors in Test set: ',  f\"{len(df_test['Donor ID'].unique()):,}\")\n",
    "\n",
    "\n",
    "print('# Donors in both Train and Test sets - the ones we choose for evaluation: ',\n",
    "      colored(f\"{len(df_train[df_train['Donor ID'].isin(df_test['Donor ID'].values.tolist())]['Donor ID'].unique()):,}\", 'blue'))\n",
    "\n",
    "\n",
    "\n",
    "df_train = df_train[df_train['Donor ID'].isin(df_test['Donor ID'].values.tolist())].reset_index(drop=True)\n",
    "df_test = df_test[df_test['Donor ID'].isin(df_train['Donor ID'].values.tolist())].reset_index(drop=True)\n",
    "\n",
    "# sum of donation in a grouped by donor id dataset\n",
    "df_main_donor_index = df.groupby(by = ['Donor ID', 'Project ID']).sum()[['Donation Amount']].reset_index().set_index('Donor ID')\n",
    "df_train_donor_index = df_train.groupby(by = ['Donor ID', 'Project ID']).sum()[['Donation Amount']].reset_index().set_index('Donor ID')\n",
    "df_test_donor_index = df_test.groupby(by = ['Donor ID', 'Project ID']).sum()[['Donation Amount']].reset_index().set_index('Donor ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = projects_df[projects_df['Project ID'].isin(df['Project ID'])].reset_index(drop=True)\n",
    "\n",
    "project_txt = projects_df.loc[:, 'project_txt']\n",
    "projects_id = projects_df['Project ID'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings (projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- embeddings---------------------#\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1') # , device= 'cuda'\n",
    "embeddings = model.encode(project_txt, batch_size=256, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EMBEDDING_PATH, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings (donors profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_PATH = 'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\Oct_28_project_embeddings.pickle'\n",
    "project_embeddings_matrix = pd.read_pickle(EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Embeddings ==================== #\n",
    "\n",
    "# ----------------------- projects profiles------------------- #\n",
    "def get_project_profile_emb(project_id: str, project_embeddings_matrix):\n",
    "\n",
    "    # get the ids\n",
    "    idx = projects_id.index(project_id)\n",
    "    project_profile = project_embeddings[idx:idx+1]\n",
    "    \n",
    "    return project_profile\n",
    "\n",
    "\n",
    "\n",
    "def get_projects_profiles_emb(ids: pd.Series, project_embeddings_matrix):\n",
    "\n",
    "\n",
    "    profiles_list = [get_project_profile_emb(project_id, project_embeddings)[0] for project_id in np.ravel([ids])]\n",
    "    project_profiles = np.vstack(profiles_list)\n",
    "\n",
    "    return project_profiles\n",
    "\n",
    "\n",
    "# ----------------------- Donors profiles------------------- #\n",
    "\n",
    "def build_donors_profile_emb(donor_id: str, df_train_donor_index: pd.DataFrame):\n",
    "\n",
    "    # get the id of each person and the projects they\n",
    "    # donated to\n",
    "    donations_donor_df = df_train_donor_index.loc[donor_id]\n",
    "\n",
    "\n",
    "    # get the vectors of projects this person has donated to\n",
    "    donor_donated_project_intrain_profiles = get_projects_profiles_emb(donations_donor_df['Project ID'], project_embeddings)\n",
    "\n",
    "\n",
    "    # get the smoothed donated amount as the weight of each project\n",
    "    donor_project_strengths = np.array(donations_donor_df['Donation Amount']).reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # multiply the weights and tfidf vectors\n",
    "    multiplication = np.multiply(donor_donated_project_intrain_profiles, donor_project_strengths)\n",
    "\n",
    "\n",
    "    # now we normalize the whole vector \n",
    "    normalized_donor_preference = preprocessing.normalize(np.sum(multiplication, axis=0).reshape(1, -1))\n",
    "    \n",
    "\n",
    "\n",
    "    return normalized_donor_preference\n",
    "\n",
    "\n",
    "def build_donors_profiles_emb(df_test_donor_index: pd.DataFrame, df_train_donor_index: pd.DataFrame, sample_size: int = None):\n",
    "    \n",
    "    # now for all donors we build a profile in a dictionary\n",
    "    donor_profiles = {}\n",
    "    donors_in_test_set = df_test_donor_index.index.unique().tolist()[:sample_size]\n",
    "\n",
    "    for donor_id in tqdm(donors_in_test_set[:sample_size], position=0, leave=True):\n",
    "        donor_profiles[donor_id] = build_donors_profile_emb(donor_id, df_train_donor_index)\n",
    "\n",
    "    return donor_profiles\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:00<00:00, 1377.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "donor_profiles = build_donors_profiles_emb(df_test_donor_index, df_train_donor_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(donor_profiles_emb['009d5fc7b87883ffad248db5150bf1fc'][0]) == 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_EMBEDDING_PATH = 'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\Oct_28_donor_profiles_embeddings.pickle'\n",
    "\n",
    "with open(PROFILE_EMBEDDING_PATH, 'wb') as f:\n",
    "    pickle.dump(donor_profiles_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_EMBEDDING_PATH = r'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\Oct_28_donor_profiles_embeddings.pickle'\n",
    "PROJECT_EMBEDDING_PATH = r'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\Oct_28_project_embeddings.pickle'\n",
    "USER_PROFILE_PATH = r'D:\\Papers\\Paper 3 - Recommender Systems\\Recommender-systems\\Files\\Graph_based\\user_profile.csv'\n",
    "\n",
    "project_embeddings = pd.read_pickle(PROJECT_EMBEDDING_PATH)\n",
    "donor_profiles_emb = pd.read_pickle(PROFILE_EMBEDDING_PATH)\n",
    "user_profile_df = pd.read_csv(USER_PROFILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "donors_id = list(donor_profiles_emb.keys())# donor_profiles_emb = user_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction matrixes for similarity scores-------------------------------------\n",
    "\n",
    "#project\n",
    "project_interactions = linear_kernel(project_embeddings_matrix, project_embeddings_matrix)\n",
    "\n",
    "# donors\n",
    "donor_embeddings_matrix = np.array([donor_profiles_emb[id][0] for id in donors_id])\n",
    "donors_interactions  = linear_kernel(donor_embeddings_matrix, donor_embeddings_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOR - DONOR / PROJ-PROJ edges dict -----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_edges(ids: list, interaction_matrix: np.array, top_n_similar: int = 20) -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    output = {'donor_id': [(score, similar_donor_id)]}\n",
    "    or \n",
    "    output = {'project_id': [(score, similar_project_id)]}\n",
    "\n",
    "    \"\"\"\n",
    "    edges_dict = {}\n",
    "    for idx, emb in enumerate(ids):\n",
    "        similar_indices = interaction_matrix[idx].argsort()[:-top_n_similar:-1]\n",
    "        \n",
    "        # idx = index of selected donor/project\n",
    "        # i = index of similar donor/project\n",
    "        # similar_items = list(score, similar donor_id/project_id)\n",
    "        similar_items = [(interaction_matrix[idx][i], ids[i]) for i in similar_indices]\n",
    "        edges_dict[ids[idx]] = similar_items\n",
    "\n",
    "    return edges_dict\n",
    "\n",
    "\n",
    "donors_edges = create_edges(donors_id, donors_interactions)\n",
    "project_edges = create_edges(projects_id, project_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs -----------------------------------------------------------\n",
    "\n",
    "# Donor graph---------------------------------------------------------------\n",
    "\n",
    "class Graph():\n",
    "    \"\"\"\n",
    "    Create the donors'/projects' graph and save their information in nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, graph_name: str):\n",
    "        self.graph = {}\n",
    "        self.graph_name = graph_name\n",
    "    \n",
    "    # function to add new nodes in the graph\n",
    "    def _create_node(self, node_id: str, node_properties):\n",
    "        self.graph[node_id] = node_properties \n",
    "    \n",
    "    # function to view the nodes in the graph\n",
    "    def _view_nodes(self):\n",
    "        return self.graph\n",
    "    \n",
    "    # function to create edges\n",
    "    def _create_edges(self, node_id, node_edges):\n",
    "        if node_id in self.graph:\n",
    "            self.graph[node_id]['edges'] = node_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donor ID                                         009d5fc7b87883ffad248db5150bf1fc\n",
       "subject_categories              {'Literacy & Language': 189, 'Music & The Arts...\n",
       "grades                          {'Grades 9-12': 17, 'Grades PreK-2': 195, 'Gra...\n",
       "metro_types                                       {'urban': 288, 'suburban': 105}\n",
       "states_donated_to                                                  {'Texas': 393}\n",
       "donated_projects                004a152bbe8952ea5e9d5ef89c179933,004a152bbe895...\n",
       "donor_zip_first_three_digits                                                  782\n",
       "donor_kmeans_cluster                                                            1\n",
       "donation_amount_sum                                                     200.95590\n",
       "donation_amount_median                                                    0.44020\n",
       "project_cost_median                                                     591.50000\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donor_id = user_profile_df.loc[0, 'Donor ID']\n",
    "next(user_profile_df[user_profile_df['Donor ID'] == donor_id].iterrows())[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize the donors graph\n",
    "dg = Graph(graph_name = 'donor')\n",
    "pg = Graph(graph_name = 'projects')\n",
    "\n",
    "\n",
    "## NODES ------------------------------------------------------------------------------------------\n",
    "# donors\n",
    "for donor_id in donors_id:\n",
    "    # donor_id = node_id\n",
    "    node_properties = dict(next(user_profile_df[user_profile_df['Donor ID'] == donor_id].iterrows())[1])\n",
    "    node_properties['donor_project_emb'] = donor_profiles_emb[donor_id][0]\n",
    "    \n",
    "    dg._create_node(donor_id, node_properties)\n",
    "\n",
    "\n",
    "# projects\n",
    "for idx, project_id in enumerate(projects_id):\n",
    "    # donor_id = node_id\n",
    "    node_properties = dict(next(projects_df[projects_df['Project ID'] == project_id].iterrows())[1])\n",
    "    node_properties['project_embedding'] = project_embeddings_matrix[idx]\n",
    "\n",
    "    del node_properties['Project Need Statement']\n",
    "    #if idx==0: print(node_properties)\n",
    "    pg._create_node(project_id, node_properties)\n",
    "\n",
    "\n",
    "\n",
    "# EDGES -----------------------------------------------------------------\n",
    "\n",
    "def create_edges_in_graph(graph: object, edges: dict, ids:list, top_n_similar: int = 5):\n",
    "\n",
    "    for id in ids:\n",
    "        node_edges = edges[id][:top_n_similar]\n",
    "        graph._create_edges(id, node_edges)\n",
    "\n",
    "create_edges_in_graph(graph = dg, edges = donors_edges, ids= donors_id, top_n_similar = 5)\n",
    "create_edges_in_graph(graph = pg, edges = project_edges, ids= projects_id, top_n_similar = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dg._view_nodes()[donor_id]\n",
    "#pg._view_nodes()[project_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5e48f84046969b800ff52f6d80523bcd1ca3fb1a99f1449e4197bf6c73dc096"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
